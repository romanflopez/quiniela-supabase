# üé∞ Backoffice - Panel de Administraci√≥n de Scrapers

## üìã Descripci√≥n

Panel de administraci√≥n web para ejecutar manualmente los scrapers de la Quiniela sin necesidad de acceder a GitHub Actions o ejecutar scripts locales.

## üöÄ Caracter√≠sticas

### ‚úÖ **Control Manual de Scrapers**
- 6 botones para ejecutar cada scraper individualmente:
  - üî§ **Ciudad** (con letras)
  - üåÖ **La Previa**
  - üåÑ **Primera**
  - ‚òÄÔ∏è **Matutina**
  - üåÜ **Vespertina**
  - üåô **Nocturna**

### ‚úÖ **UI Moderna y Responsive**
- Dise√±o tipo admin panel con efectos visuales
- Cards animadas con estados (idle, running, success, error)
- Efectos de pulso cuando un scraper est√° corriendo
- Colores: negro, rojo, dorado, verde ne√≥n

### ‚úÖ **Logs en Tiempo Real**
- Panel de logs actualizado en tiempo real
- Timestamp en cada log
- Colores por tipo: info, success, error, warning
- Bot√≥n para limpiar logs
- Auto-scroll a los logs m√°s recientes

### ‚úÖ **Estados Visuales**
- **Idle** (gris): Listo para ejecutar
- **Running** (dorado pulsante): Scraper en ejecuci√≥n
- **Success** (verde): Scraping exitoso
- **Error** (rojo): Fall√≥ el scraping

## üîß Configuraci√≥n

### Opci√≥n 1: Usar con Edge Function (Recomendado)

1. **Abrir `backoffice.html`** en cualquier navegador
2. Los scrapers llamar√°n a la Edge Function de Supabase
3. La Edge Function debe estar desplegada:
   ```bash
   supabase functions deploy quiniela-scraper
   ```
4. No requiere configuraci√≥n adicional

### Opci√≥n 2: Modo Local (Alternativo)

Para usar scrapers locales en lugar de Edge Functions:

1. Crear un servidor simple en Node.js:

```javascript
// scripts/backoffice-server.js
import express from 'express';
import { spawn } from 'child_process';
import cors from 'cors';

const app = express();
app.use(cors());
app.use(express.json());

app.post('/api/scrape', (req, res) => {
    const { turno } = req.body;
    
    const scraperMap = {
        'ciudad': 'scraper-ciudad.js',
        'laprevia': 'scraper-by-turno.js',
        'primera': 'scraper-by-turno.js',
        'matutina': 'scraper-by-turno.js',
        'vespertina': 'scraper-by-turno.js',
        'nocturna': 'scraper-by-turno.js'
    };
    
    const script = scraperMap[turno];
    const args = turno === 'ciudad' ? [] : [turno];
    
    const process = spawn('node', [script, ...args], {
        cwd: __dirname
    });
    
    let output = '';
    
    process.stdout.on('data', (data) => {
        output += data.toString();
    });
    
    process.on('close', (code) => {
        if (code === 0) {
            res.json({
                status: 'ok',
                sorteos_guardados: 1,
                output: output
            });
        } else {
            res.status(500).json({
                status: 'error',
                details: 'Scraper fall√≥',
                output: output
            });
        }
    });
});

app.listen(3000, () => {
    console.log('Backoffice server running on port 3000');
});
```

2. Instalar dependencias:
   ```bash
   cd scripts
   npm install express cors
   ```

3. Ejecutar el servidor:
   ```bash
   node backoffice-server.js
   ```

4. Modificar `backoffice.html` l√≠nea 350:
   ```javascript
   const SCRAPER_FUNCTION = 'http://localhost:3000/api/scrape';
   ```

## üìñ Uso

### Ejecutar un Scraper

1. Abrir `backoffice.html` en el navegador
2. Click en el bot√≥n "‚ñ∂ Ejecutar Scraper" del turno deseado
3. El card mostrar√° estado "Running" con animaci√≥n
4. Ver logs en tiempo real en el panel inferior
5. Resultado:
   - ‚úÖ Verde = Exitoso
   - ‚ùå Rojo = Fall√≥

### Leer los Logs

Los logs muestran:
- `[HH:MM:SS]` - Timestamp
- Mensaje de estado
- Color seg√∫n tipo:
  - **Verde**: √âxito
  - **Rojo**: Error
  - **Amarillo**: Warning
  - **Blanco**: Info

### Limpiar Logs

Click en el bot√≥n "üóëÔ∏è Limpiar Logs" en el panel de logs.

## üéØ Casos de Uso

### 1. **Sorteo Fall√≥ en GitHub Actions**
- Abrir backoffice
- Ejecutar el scraper espec√≠fico manualmente
- Verificar el resultado

### 2. **Quiero Forzar un Scraping Ahora**
- No esperar al horario programado
- Ejecutar desde backoffice
- Datos disponibles de inmediato

### 3. **Probar si un Scraper Funciona**
- Ejecutar desde backoffice
- Ver logs detallados
- Debugging r√°pido

### 4. **Ciudad Est√° Dando Problemas**
- Ejecutar scraper de Ciudad
- Ver intentos y errores en tiempo real
- Ajustar sorteo_id si es necesario

## üîç Troubleshooting

### El bot√≥n no hace nada

**Problema**: CORS bloqueado

**Soluci√≥n**:
1. Verificar que la Edge Function tenga CORS habilitado
2. O usar modo local con servidor Node.js

### Error: "Failed to fetch"

**Problema**: Edge Function no desplegada o URL incorrecta

**Soluci√≥n**:
1. Verificar URL en l√≠nea 350 de `backoffice.html`
2. Desplegar Edge Function:
   ```bash
   supabase functions deploy quiniela-scraper
   ```

### Ciudad Siempre Falla

**Problema**: Ciudad es hist√≥ricamente problem√°tico

**Soluci√≥n**:
- Normal, API de LOTBA de Ciudad es inestable
- Intentar varias veces
- Verificar logs para ver detalles
- Probar con sorteo_id diferente

### Scraper se queda en "Running" forever

**Problema**: Edge Function timeout o error

**Soluci√≥n**:
1. Refrescar la p√°gina
2. Ver logs de la Edge Function:
   ```bash
   supabase functions logs quiniela-scraper --tail
   ```
3. Verificar DATABASE_URL

## üé® Personalizaci√≥n

### Cambiar URL de la API

Editar l√≠nea 350 en `backoffice.html`:

```javascript
const SCRAPER_FUNCTION = 'TU_URL_AQUI';
```

### Agregar Nuevos Scrapers

1. Agregar card en el HTML:
```html
<div class="scraper-card" id="card-nuevoscraper">
    <div class="scraper-header">
        <div class="scraper-title">üÜï Nuevo Scraper</div>
        <div class="scraper-icon">‚≠ê</div>
    </div>
    <!-- resto del card -->
</div>
```

2. Agregar en el objeto `scraperNames` del JavaScript

3. Actualizar Edge Function para manejar el nuevo scraper

### Modificar Estilos

Todas las variables de color est√°n en el `<style>`:
- Background: `#0a0a0a` a `#1a1a2e`
- Primario: `#ffd700` (dorado)
- Success: `#00ff88` (verde ne√≥n)
- Error: `#ff6b6b` (rojo)
- Running: `#ffd700` (amarillo)

## üìä Ventajas vs GitHub Actions

| Caracter√≠stica | Backoffice | GitHub Actions |
|----------------|------------|----------------|
| **Ejecuci√≥n Manual** | ‚úÖ Inmediata | ‚è≥ Requiere login |
| **Logs en Tiempo Real** | ‚úÖ Si | ‚ùå No |
| **UI Visual** | ‚úÖ Moderna | ‚ö†Ô∏è B√°sica |
| **Debugging** | ‚úÖ F√°cil | ‚ö†Ô∏è Complicado |
| **M√≥vil** | ‚úÖ Responsive | ‚ö†Ô∏è Desktop only |
| **Programado** | ‚ùå No | ‚úÖ Cron |

## üöÄ Deploy del Backoffice

### Opci√≥n 1: GitHub Pages

1. Crear carpeta `docs/` en el repo
2. Copiar `backoffice.html` a `docs/index.html`
3. GitHub Settings ‚Üí Pages ‚Üí Source: `docs/`
4. URL: `https://tu-usuario.github.io/tu-repo/`

### Opci√≥n 2: Netlify

1. Drag & drop `backoffice.html` en Netlify
2. URL autom√°tica generada

### Opci√≥n 3: Vercel

```bash
vercel --prod
```

### Opci√≥n 4: Hosting Local

Simplemente abrir `backoffice.html` en el navegador.

## üîê Seguridad

‚ö†Ô∏è **IMPORTANTE**: El backoffice llama a funciones p√∫blicas de Supabase.

Para mayor seguridad:

1. **Agregar autenticaci√≥n** con Supabase Auth
2. **Usar API Keys** secretas
3. **Rate limiting** en Edge Functions
4. **IP whitelist** si es posible

Ejemplo con API Key:

```javascript
const response = await fetch(SCRAPER_FUNCTION, {
    method: 'POST',
    headers: {
        'Content-Type': 'application/json',
        'Authorization': 'Bearer TU_API_KEY_SECRETA'
    },
    body: JSON.stringify({ turno: scraper })
});
```

## üìù Notas

- El backoffice NO reemplaza GitHub Actions
- GitHub Actions sigue funcionando autom√°ticamente
- El backoffice es ADICIONAL para control manual
- Ideal para debugging y scrapers fallidos

## üéâ Conclusi√≥n

El backoffice te da **control total** sobre los scrapers sin necesidad de:
- Abrir terminal
- Ejecutar comandos
- Ir a GitHub Actions
- Iniciar sesi√≥n en GitHub

**Todo desde un navegador con interfaz visual moderna.** üöÄ

